{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f23710da",
   "metadata": {},
   "source": [
    "# C. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461036a",
   "metadata": {},
   "source": [
    "1. Clears the TensorFlow session and release any resources held by the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff35094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a7fe8",
   "metadata": {},
   "source": [
    "2. Adjusts the logging level for TensorFlow to suppress warnings and control the verbosity of log messages generated by TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4a18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the TensorFlow logging level to suppress warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  \n",
    "# '0' (default) shows all logs, \n",
    "#'1' shows only errors, \n",
    "#'2' shows only errors and critical logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42fecd",
   "metadata": {},
   "source": [
    "3. Import libraries/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563a8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# numpy library for numerical computations and array operations\n",
    "import numpy as np\n",
    "\n",
    "#time module for various time-related functions eg: adding delays or timestamping \n",
    "import time\n",
    "\n",
    "# seaborn library, which is a data visualization library built on top of matplotlib. \n",
    "# for creating attractive and informative statistical graphics\n",
    "import seaborn as sns\n",
    "\n",
    "# pyplot module from the matplotlib library \n",
    "# provides a MATLAB-like plotting interface for creating static, interactive, and animated visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prevents overfitting in neural network models\n",
    "# used for regularization techniques l1_l2\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Dropout: improves the model's generalization by reducing the reliance on specific neurons \n",
    "#          and helps prevent overfitting\n",
    "# Reshape: modifies the dimensions of the input data before passing it to subsequent layers\n",
    "# LSTM: for sequence modeling tasks and time-series data\n",
    "# Dense: applies a linear transformation to the input data followed by an activation function\n",
    "from tensorflow.keras.layers import Dropout, Reshape, LSTM, Dense \n",
    "\n",
    "# creates a linear stack of layers for building sequential neural network models\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "# converts class labels or target values into one-hot encoded format\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# monitors a specified metric and stops the training process early if the metric's improvement \n",
    "#    stagnates or deteriorates over a certain number of epochs\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# for compiling and training the neural network model\n",
    "from tensorflow.keras.optimizers import legacy as legacy_adam\n",
    "\n",
    "# splits the dataset into training and testing subsets for model development and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# classification_report: generates a comprehensive report that includes various classification \n",
    "#                        metrics for each class in a classification problem\n",
    "# confusion_matrix: creates a confusion matrix, which is a table that shows the count of true \n",
    "#                   positive, true negative, false positive, and false negative predictions \n",
    "#                   for each class in a classification problem\n",
    "# accuracy_score: calculates the accuracy of a classification model by comparing the true labels \n",
    "#                 with the predicted labels\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33b079",
   "metadata": {},
   "source": [
    "4. Variables used when building and training neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63de5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define variables\n",
    "num_sensors = 5 # number of flex sensors\n",
    "num_classes = 10 # number of gestures\n",
    "\n",
    "# Define training parameters\n",
    "# number of times the entire dataset is passed through the neural network during training\n",
    "epochs = 1000 \n",
    "\n",
    "# number of training sets utilized in one iteration\n",
    "batch_size = 128 \n",
    "\n",
    "# rate at which the model learns from the data\n",
    "learning_rate = 0.0001 \n",
    "\n",
    "# initializes the optimizer used to update the model's weights during training\n",
    "optimizer = legacy_adam.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc521c2",
   "metadata": {},
   "source": [
    "5. RNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6cb3039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    }
   ],
   "source": [
    "# creates an instance of the Sequential class to stack layers in a linear manner\n",
    "model = Sequential()\n",
    "\n",
    "# first layer: LSTM layer with 1024 units, variable length with num_sensors features \n",
    "#              and returns the full sequence of outputs for each time step\n",
    "model.add(LSTM(1024, return_sequences=True, input_shape=(None, num_sensors )))\n",
    "\n",
    "# second layer: LSTM layer with 512 units maintaining the sequence output\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "\n",
    "# third layer: Dropout layer with a dropout rate of 0.5, helps prevent overfitting by \n",
    "#              randomly setting a fraction of input units to 0 during each update during training\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# fourth layer: LSTM layer with 512 units maintaining the sequence output\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "\n",
    "# fifth layer: LSTM layer with 512 units, returns only the final output of the sequence\n",
    "model.add(LSTM(512))\n",
    "\n",
    "# sixth layer: Dropout layer with a dropout rate of 0.5\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# seventh layer: Dense (fully connected) layer with 32 units, Leaky ReLU activation \n",
    "#                including L1 and L2 regularization with values of 0 and 0.001, respectively.\n",
    "model.add(Dense(32, activation='LeakyReLU', kernel_regularizer=regularizers.l1_l2(l1=0, l2=0.001)))\n",
    "\n",
    "# eighth layer: Dropout layer with a dropout rate of 0.5\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# final layer: Dense layer with num_classes units and a softmax activation function\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054515de",
   "metadata": {},
   "source": [
    "6. Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9cef220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af88d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 1024)        4218880   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         3147776   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 512)         0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 512)         2099200   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16416     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11581802 (44.18 MB)\n",
      "Trainable params: 11581802 (44.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c5b82",
   "metadata": {},
   "source": [
    "7. Perform data preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bce9e",
   "metadata": {},
   "source": [
    "a. Handling Missing Values: Check for missing values and handle them if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85839d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thumb              0\n",
       "IndexFinger        0\n",
       "MiddleFinger       0\n",
       "RingFinger         0\n",
       "LittleFinger       0\n",
       "accelerometer_x    0\n",
       "accelerometer_y    0\n",
       "accelerometer_z    0\n",
       "temperature        0\n",
       "gyro_x             0\n",
       "gyro_y             0\n",
       "gyro_z             0\n",
       "Gesture            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads the CSV file and assigns the data to a DataFrame\n",
    "df = pd.read_csv('/Users/sunilamaharjan/Desktop/London Metropolitan University/Summer Semester/mscProject/codes/Dataset/Number/sensor_data.csv')\n",
    "\n",
    "# returns the count of missing values for each column in the DataFrame\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79685e",
   "metadata": {},
   "source": [
    "b. One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf402547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Thumb  IndexFinger  MiddleFinger  RingFinger  LittleFinger  \\\n",
      "0       442          369           455         406           353   \n",
      "1       485          461           491         438           407   \n",
      "2       446          450           492         409           399   \n",
      "3       385          445           489         413           413   \n",
      "4       429          374           451         383           441   \n",
      "...     ...          ...           ...         ...           ...   \n",
      "7074    348          392           364         413           409   \n",
      "7075    350          394           364         406           405   \n",
      "7076    351          394           363         407           401   \n",
      "7077    351          394           363         408           402   \n",
      "7078    351          394           364         408           402   \n",
      "\n",
      "      accelerometer_x  accelerometer_y  accelerometer_z  temperature  gyro_x  \\\n",
      "0               21728             1664             2208        28.01   31484   \n",
      "1               15512            -1572             9576        28.11  -17512   \n",
      "2               13984            15168            10272        28.11  -12953   \n",
      "3                9200             8696             1392        28.06   -6873   \n",
      "4               32360             3016             2720        28.06  -16065   \n",
      "...               ...              ...              ...          ...     ...   \n",
      "7074             6784            14004             5216        30.69     -93   \n",
      "7075             7072            13352             6428        30.69   -2836   \n",
      "7076             6976            12672             7964        30.69    -628   \n",
      "7077             7016            13336             7716        30.65    -608   \n",
      "7078             6964            13340             7840        30.65    -593   \n",
      "\n",
      "      ...  Gesture_0  Gesture_1  Gesture_2  Gesture_3  Gesture_4  Gesture_5  \\\n",
      "0     ...          1          0          0          0          0          0   \n",
      "1     ...          1          0          0          0          0          0   \n",
      "2     ...          1          0          0          0          0          0   \n",
      "3     ...          1          0          0          0          0          0   \n",
      "4     ...          1          0          0          0          0          0   \n",
      "...   ...        ...        ...        ...        ...        ...        ...   \n",
      "7074  ...          0          0          0          0          0          0   \n",
      "7075  ...          0          0          0          0          0          0   \n",
      "7076  ...          0          0          0          0          0          0   \n",
      "7077  ...          0          0          0          0          0          0   \n",
      "7078  ...          0          0          0          0          0          0   \n",
      "\n",
      "      Gesture_6  Gesture_7  Gesture_8  Gesture_9  \n",
      "0             0          0          0          0  \n",
      "1             0          0          0          0  \n",
      "2             0          0          0          0  \n",
      "3             0          0          0          0  \n",
      "4             0          0          0          0  \n",
      "...         ...        ...        ...        ...  \n",
      "7074          0          0          0          1  \n",
      "7075          0          0          0          1  \n",
      "7076          0          0          0          1  \n",
      "7077          0          0          0          1  \n",
      "7078          0          0          0          1  \n",
      "\n",
      "[7079 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Performs one-hot encoding on the 'Gesture' column\n",
    "one_hot_encoded = pd.get_dummies(df['Gesture'], prefix='Gesture')\n",
    "\n",
    "# Concatenates the encoded columns with the original DataFrame\n",
    "df_encoded = pd.concat([df, one_hot_encoded], axis=1)\n",
    "\n",
    "# Removes the original 'Gesture' column if desired\n",
    "df_encoded = df_encoded.drop('Gesture', axis=1)\n",
    "\n",
    "# Prints the encoded DataFrame\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8632da",
   "metadata": {},
   "source": [
    "c. Prepare Features X and labels y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b5e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Thumb</th>\n",
       "      <th>IndexFinger</th>\n",
       "      <th>MiddleFinger</th>\n",
       "      <th>RingFinger</th>\n",
       "      <th>LittleFinger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>369</td>\n",
       "      <td>455</td>\n",
       "      <td>406</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>485</td>\n",
       "      <td>461</td>\n",
       "      <td>491</td>\n",
       "      <td>438</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>446</td>\n",
       "      <td>450</td>\n",
       "      <td>492</td>\n",
       "      <td>409</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>385</td>\n",
       "      <td>445</td>\n",
       "      <td>489</td>\n",
       "      <td>413</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>429</td>\n",
       "      <td>374</td>\n",
       "      <td>451</td>\n",
       "      <td>383</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7074</th>\n",
       "      <td>348</td>\n",
       "      <td>392</td>\n",
       "      <td>364</td>\n",
       "      <td>413</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7075</th>\n",
       "      <td>350</td>\n",
       "      <td>394</td>\n",
       "      <td>364</td>\n",
       "      <td>406</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>351</td>\n",
       "      <td>394</td>\n",
       "      <td>363</td>\n",
       "      <td>407</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>351</td>\n",
       "      <td>394</td>\n",
       "      <td>363</td>\n",
       "      <td>408</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>351</td>\n",
       "      <td>394</td>\n",
       "      <td>364</td>\n",
       "      <td>408</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7079 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Thumb  IndexFinger  MiddleFinger  RingFinger  LittleFinger\n",
       "0       442          369           455         406           353\n",
       "1       485          461           491         438           407\n",
       "2       446          450           492         409           399\n",
       "3       385          445           489         413           413\n",
       "4       429          374           451         383           441\n",
       "...     ...          ...           ...         ...           ...\n",
       "7074    348          392           364         413           409\n",
       "7075    350          394           364         406           405\n",
       "7076    351          394           363         407           401\n",
       "7077    351          394           363         408           402\n",
       "7078    351          394           364         408           402\n",
       "\n",
       "[7079 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops columns that contain the word 'Gesture' in their name which was been created after one-hot encoded\n",
    "X = df_encoded.drop(columns=df_encoded.filter(like='Gesture').columns, axis=1)\n",
    "\n",
    "# dropping all other unnecessary columns\n",
    "X = X.drop('temperature', axis=1)\n",
    "X = X.drop('accelerometer_x', axis=1)\n",
    "X = X.drop('accelerometer_y', axis=1)\n",
    "X = X.drop('accelerometer_z', axis=1)\n",
    "X = X.drop('gyro_x', axis=1)\n",
    "X = X.drop('gyro_y', axis=1)\n",
    "X = X.drop('gyro_z', axis=1)\n",
    "\n",
    "# extracts the column with the highest value (1) from the one-hot encoded gesture columns using the \n",
    "# idxmax(axis=1) function\n",
    "y = df_encoded[df_encoded.filter(like='Gesture').columns].idxmax(axis=1)\n",
    "\n",
    "# processes the extracted gesture labels by splitting the string on underscores ('_') and converting \n",
    "# the second part to an integer.\n",
    "y = y.str.split('_').str[1].astype(int)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5988a387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "7074    9\n",
       "7075    9\n",
       "7076    9\n",
       "7077    9\n",
       "7078    9\n",
       "Length: 7079, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a80e4",
   "metadata": {},
   "source": [
    "8. Splitting the Dataset into training, validation, and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78fa580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4530, 5) (1416, 5) (4530,) (1416,) (1133, 5) (1133,)\n"
     ]
    }
   ],
   "source": [
    "# Splits feature matrix X and label vector y into training and testing sets with proportion of data \n",
    "# to be used for testing, 20%, ensuring the data split is reproducible\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# further splits the training data (X_train and y_train) into training and validation sets with \n",
    "# proportion of 20% ensuring the data split is reproducible\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape,X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd401464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[340, 498, 474, 274, 317]],\n",
       "\n",
       "       [[363, 470, 375, 289, 316]],\n",
       "\n",
       "       [[336, 278, 363, 272, 297]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[426, 500, 490, 261, 287]],\n",
       "\n",
       "       [[338, 472, 348, 247, 278]],\n",
       "\n",
       "       [[454, 460, 493, 269, 297]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model expects a 3-dimensional input tensor, but the provided input data has a shape of (32, 11), \n",
    "# which is 2-dimensional.\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_train_array = X_train.values\n",
    "X_val_array = X_val.values\n",
    "\n",
    "# Reshape the training data [(num_samples, 1, num_features)]\n",
    "X_train_reshaped = X_train.values.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "\n",
    "# Reshape the validation data\n",
    "X_val_reshaped = X_val.values.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "# Reshape the testing data\n",
    "X_test_reshaped = X_test.values.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "X_train_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67271980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4530, 1, 5) (1133, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_reshaped.shape, X_val_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee618d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4530,) (1133,) (1416,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c32a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's output shape is (batch_size, num_classes), which in this case is (32, 10), \n",
    "# but the shape of the target values is (32, 1).\n",
    "\n",
    "# Convert the target values to one-hot encoded format\n",
    "y_train_encoded = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_encoded = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Remove the extra dimension from the target data\n",
    "y_train_encoded = np.squeeze(y_train_encoded)\n",
    "y_val_encoded = np.squeeze(y_val_encoded)\n",
    "y_test_encoded = np.squeeze(y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789f988",
   "metadata": {},
   "source": [
    "# D. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84a7476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "36/36 [==============================] - 4s 58ms/step - loss: 2.3513 - accuracy: 0.1929 - val_loss: 2.3294 - val_accuracy: 0.3186\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 2.2423 - accuracy: 0.2561 - val_loss: 2.0867 - val_accuracy: 0.3177\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 1.9524 - accuracy: 0.3347 - val_loss: 1.7157 - val_accuracy: 0.5004\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.6993 - accuracy: 0.4307 - val_loss: 1.5184 - val_accuracy: 0.5128\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 1.5576 - accuracy: 0.4808 - val_loss: 1.3877 - val_accuracy: 0.5163\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 1.4676 - accuracy: 0.5102 - val_loss: 1.3074 - val_accuracy: 0.5534\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.4053 - accuracy: 0.5201 - val_loss: 1.2378 - val_accuracy: 0.5940\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.3368 - accuracy: 0.5444 - val_loss: 1.1763 - val_accuracy: 0.6055\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 1.2758 - accuracy: 0.5634 - val_loss: 1.1193 - val_accuracy: 0.6108\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 1.2408 - accuracy: 0.5766 - val_loss: 1.0660 - val_accuracy: 0.6646\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.2041 - accuracy: 0.5823 - val_loss: 1.0319 - val_accuracy: 0.6681\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.1529 - accuracy: 0.6026 - val_loss: 1.0136 - val_accuracy: 0.6523\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 1.1339 - accuracy: 0.6031 - val_loss: 0.9630 - val_accuracy: 0.6664\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.0888 - accuracy: 0.6223 - val_loss: 0.9341 - val_accuracy: 0.6823\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 1.0743 - accuracy: 0.6214 - val_loss: 0.9776 - val_accuracy: 0.6496\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.0262 - accuracy: 0.6397 - val_loss: 0.9013 - val_accuracy: 0.6770\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 1.0335 - accuracy: 0.6302 - val_loss: 0.9310 - val_accuracy: 0.6611\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 1.0194 - accuracy: 0.6369 - val_loss: 0.8913 - val_accuracy: 0.6637\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 1.0063 - accuracy: 0.6342 - val_loss: 0.8754 - val_accuracy: 0.6787\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9815 - accuracy: 0.6521 - val_loss: 0.8754 - val_accuracy: 0.6955\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.9818 - accuracy: 0.6503 - val_loss: 0.9203 - val_accuracy: 0.6593\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9653 - accuracy: 0.6481 - val_loss: 0.8305 - val_accuracy: 0.6884\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9398 - accuracy: 0.6552 - val_loss: 0.8602 - val_accuracy: 0.6699\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9363 - accuracy: 0.6570 - val_loss: 0.8315 - val_accuracy: 0.6805\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.9148 - accuracy: 0.6687 - val_loss: 0.8158 - val_accuracy: 0.6929\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9113 - accuracy: 0.6684 - val_loss: 0.8106 - val_accuracy: 0.6717\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.9003 - accuracy: 0.6715 - val_loss: 0.7945 - val_accuracy: 0.7096\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8905 - accuracy: 0.6689 - val_loss: 0.8110 - val_accuracy: 0.7017\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8893 - accuracy: 0.6775 - val_loss: 0.7965 - val_accuracy: 0.6981\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8755 - accuracy: 0.6821 - val_loss: 0.7855 - val_accuracy: 0.7105\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8665 - accuracy: 0.6854 - val_loss: 0.7854 - val_accuracy: 0.7193\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8705 - accuracy: 0.6746 - val_loss: 0.7950 - val_accuracy: 0.6973\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.8773 - accuracy: 0.6751 - val_loss: 0.8161 - val_accuracy: 0.7008\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.8583 - accuracy: 0.6821 - val_loss: 0.7799 - val_accuracy: 0.7167\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.8366 - accuracy: 0.6934 - val_loss: 0.7627 - val_accuracy: 0.6999\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.8392 - accuracy: 0.6901 - val_loss: 0.7585 - val_accuracy: 0.7167\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.8497 - accuracy: 0.6852 - val_loss: 0.7616 - val_accuracy: 0.6990\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.8438 - accuracy: 0.6887 - val_loss: 0.7786 - val_accuracy: 0.7184\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8233 - accuracy: 0.6945 - val_loss: 0.7546 - val_accuracy: 0.7061\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8291 - accuracy: 0.6857 - val_loss: 0.7549 - val_accuracy: 0.6946\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8103 - accuracy: 0.6996 - val_loss: 0.7364 - val_accuracy: 0.7176\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8121 - accuracy: 0.6958 - val_loss: 0.7341 - val_accuracy: 0.7308\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8175 - accuracy: 0.6976 - val_loss: 0.7139 - val_accuracy: 0.7405\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7879 - accuracy: 0.7033 - val_loss: 0.7425 - val_accuracy: 0.6920\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.8036 - accuracy: 0.6934 - val_loss: 0.7394 - val_accuracy: 0.7061\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7885 - accuracy: 0.7029 - val_loss: 0.7189 - val_accuracy: 0.7237\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.8052 - accuracy: 0.6971 - val_loss: 0.7076 - val_accuracy: 0.7440\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7883 - accuracy: 0.6991 - val_loss: 0.7500 - val_accuracy: 0.7140\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.7881 - accuracy: 0.7073 - val_loss: 0.7080 - val_accuracy: 0.7335\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7747 - accuracy: 0.6998 - val_loss: 0.7290 - val_accuracy: 0.6999\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.7780 - accuracy: 0.7009 - val_loss: 0.7397 - val_accuracy: 0.7052\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7822 - accuracy: 0.7055 - val_loss: 0.7019 - val_accuracy: 0.7246\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7731 - accuracy: 0.7097 - val_loss: 0.7015 - val_accuracy: 0.7140\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.7695 - accuracy: 0.7066 - val_loss: 0.7348 - val_accuracy: 0.7034\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7663 - accuracy: 0.7064 - val_loss: 0.7075 - val_accuracy: 0.7123\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7556 - accuracy: 0.7119 - val_loss: 0.7289 - val_accuracy: 0.7255\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7698 - accuracy: 0.7084 - val_loss: 0.7284 - val_accuracy: 0.7202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7614 - accuracy: 0.7068 - val_loss: 0.6882 - val_accuracy: 0.7387\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7390 - accuracy: 0.7201 - val_loss: 0.7061 - val_accuracy: 0.7070\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7534 - accuracy: 0.7115 - val_loss: 0.6771 - val_accuracy: 0.7476\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7511 - accuracy: 0.7155 - val_loss: 0.6843 - val_accuracy: 0.7184\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7454 - accuracy: 0.7163 - val_loss: 0.6925 - val_accuracy: 0.7308\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7300 - accuracy: 0.7199 - val_loss: 0.6724 - val_accuracy: 0.7282\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7283 - accuracy: 0.7227 - val_loss: 0.6996 - val_accuracy: 0.7255\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.7365 - accuracy: 0.7203 - val_loss: 0.7006 - val_accuracy: 0.7140\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7224 - accuracy: 0.7265 - val_loss: 0.6783 - val_accuracy: 0.7282\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7200 - accuracy: 0.7185 - val_loss: 0.7273 - val_accuracy: 0.7070\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.7299 - accuracy: 0.7223 - val_loss: 0.7372 - val_accuracy: 0.7132\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7388 - accuracy: 0.7139 - val_loss: 0.6863 - val_accuracy: 0.7308\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7324 - accuracy: 0.7155 - val_loss: 0.6820 - val_accuracy: 0.7343\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7384 - accuracy: 0.7179 - val_loss: 0.7408 - val_accuracy: 0.7114\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7209 - accuracy: 0.7225 - val_loss: 0.6736 - val_accuracy: 0.7467\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7144 - accuracy: 0.7245 - val_loss: 0.6785 - val_accuracy: 0.7370\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7287 - accuracy: 0.7219 - val_loss: 0.7115 - val_accuracy: 0.7361\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7150 - accuracy: 0.7241 - val_loss: 0.6700 - val_accuracy: 0.7520\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6997 - accuracy: 0.7291 - val_loss: 0.6389 - val_accuracy: 0.7626\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7144 - accuracy: 0.7302 - val_loss: 0.6679 - val_accuracy: 0.7370\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.7055 - accuracy: 0.7232 - val_loss: 0.6362 - val_accuracy: 0.7546\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7199 - accuracy: 0.7205 - val_loss: 0.6782 - val_accuracy: 0.7449\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6865 - accuracy: 0.7305 - val_loss: 0.6433 - val_accuracy: 0.7352\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6897 - accuracy: 0.7395 - val_loss: 0.6331 - val_accuracy: 0.7643\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.7051 - accuracy: 0.7289 - val_loss: 0.6600 - val_accuracy: 0.7458\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6984 - accuracy: 0.7280 - val_loss: 0.6679 - val_accuracy: 0.7140\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6974 - accuracy: 0.7296 - val_loss: 0.6301 - val_accuracy: 0.7485\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6828 - accuracy: 0.7380 - val_loss: 0.6454 - val_accuracy: 0.7511\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6980 - accuracy: 0.7333 - val_loss: 0.6526 - val_accuracy: 0.7396\n",
      "Epoch 87/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6889 - accuracy: 0.7311 - val_loss: 0.6494 - val_accuracy: 0.7405\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6832 - accuracy: 0.7291 - val_loss: 0.6349 - val_accuracy: 0.7608\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6752 - accuracy: 0.7439 - val_loss: 0.6251 - val_accuracy: 0.7476\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6908 - accuracy: 0.7307 - val_loss: 0.6856 - val_accuracy: 0.7114\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6950 - accuracy: 0.7351 - val_loss: 0.6696 - val_accuracy: 0.7379\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6722 - accuracy: 0.7384 - val_loss: 0.6381 - val_accuracy: 0.7423\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6691 - accuracy: 0.7371 - val_loss: 0.6103 - val_accuracy: 0.7546\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6524 - accuracy: 0.7472 - val_loss: 0.6476 - val_accuracy: 0.7485\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6754 - accuracy: 0.7415 - val_loss: 0.6380 - val_accuracy: 0.7635\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6518 - accuracy: 0.7479 - val_loss: 0.6072 - val_accuracy: 0.7564\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6560 - accuracy: 0.7470 - val_loss: 0.6761 - val_accuracy: 0.7432\n",
      "Epoch 98/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6548 - accuracy: 0.7517 - val_loss: 0.6014 - val_accuracy: 0.7696\n",
      "Epoch 99/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6468 - accuracy: 0.7459 - val_loss: 0.6122 - val_accuracy: 0.7723\n",
      "Epoch 100/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6543 - accuracy: 0.7492 - val_loss: 0.6837 - val_accuracy: 0.7308\n",
      "Epoch 101/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6617 - accuracy: 0.7364 - val_loss: 0.6217 - val_accuracy: 0.7538\n",
      "Epoch 102/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6614 - accuracy: 0.7347 - val_loss: 0.6303 - val_accuracy: 0.7599\n",
      "Epoch 103/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6726 - accuracy: 0.7393 - val_loss: 0.6304 - val_accuracy: 0.7696\n",
      "Epoch 104/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6523 - accuracy: 0.7450 - val_loss: 0.6313 - val_accuracy: 0.7282\n",
      "Epoch 105/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6449 - accuracy: 0.7437 - val_loss: 0.6365 - val_accuracy: 0.7502\n",
      "Epoch 106/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6494 - accuracy: 0.7477 - val_loss: 0.6124 - val_accuracy: 0.7582\n",
      "Epoch 107/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6485 - accuracy: 0.7550 - val_loss: 0.6281 - val_accuracy: 0.7626\n",
      "Epoch 108/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6380 - accuracy: 0.7486 - val_loss: 0.6152 - val_accuracy: 0.7582\n",
      "Epoch 109/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6466 - accuracy: 0.7514 - val_loss: 0.6235 - val_accuracy: 0.7467\n",
      "Epoch 110/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6278 - accuracy: 0.7541 - val_loss: 0.5825 - val_accuracy: 0.7855\n",
      "Epoch 111/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6415 - accuracy: 0.7512 - val_loss: 0.5945 - val_accuracy: 0.7776\n",
      "Epoch 112/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6315 - accuracy: 0.7512 - val_loss: 0.6055 - val_accuracy: 0.7705\n",
      "Epoch 113/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6428 - accuracy: 0.7550 - val_loss: 0.6390 - val_accuracy: 0.7688\n",
      "Epoch 114/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6466 - accuracy: 0.7536 - val_loss: 0.6264 - val_accuracy: 0.7617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6358 - accuracy: 0.7472 - val_loss: 0.6217 - val_accuracy: 0.7529\n",
      "Epoch 116/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6214 - accuracy: 0.7611 - val_loss: 0.6140 - val_accuracy: 0.7670\n",
      "Epoch 117/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6447 - accuracy: 0.7461 - val_loss: 0.5889 - val_accuracy: 0.7793\n",
      "Epoch 118/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6324 - accuracy: 0.7552 - val_loss: 0.5849 - val_accuracy: 0.7705\n",
      "Epoch 119/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6152 - accuracy: 0.7629 - val_loss: 0.6105 - val_accuracy: 0.7599\n",
      "Epoch 120/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.6090 - accuracy: 0.7667 - val_loss: 0.5822 - val_accuracy: 0.7802\n",
      "Epoch 121/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6059 - accuracy: 0.7638 - val_loss: 0.5846 - val_accuracy: 0.7802\n",
      "Epoch 122/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6181 - accuracy: 0.7640 - val_loss: 0.5791 - val_accuracy: 0.7785\n",
      "Epoch 123/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6225 - accuracy: 0.7534 - val_loss: 0.6065 - val_accuracy: 0.7723\n",
      "Epoch 124/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6320 - accuracy: 0.7523 - val_loss: 0.6490 - val_accuracy: 0.7467\n",
      "Epoch 125/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.6242 - accuracy: 0.7614 - val_loss: 0.5956 - val_accuracy: 0.7590\n",
      "Epoch 126/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6024 - accuracy: 0.7653 - val_loss: 0.5656 - val_accuracy: 0.7767\n",
      "Epoch 127/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.6051 - accuracy: 0.7662 - val_loss: 0.5658 - val_accuracy: 0.8023\n",
      "Epoch 128/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5983 - accuracy: 0.7759 - val_loss: 0.6197 - val_accuracy: 0.7714\n",
      "Epoch 129/1000\n",
      "36/36 [==============================] - 2s 45ms/step - loss: 0.6705 - accuracy: 0.7439 - val_loss: 0.5816 - val_accuracy: 0.7520\n",
      "Epoch 130/1000\n",
      "36/36 [==============================] - 2s 49ms/step - loss: 0.6104 - accuracy: 0.7640 - val_loss: 0.5916 - val_accuracy: 0.7891\n",
      "Epoch 131/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.5959 - accuracy: 0.7627 - val_loss: 0.6037 - val_accuracy: 0.7564\n",
      "Epoch 132/1000\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 0.5952 - accuracy: 0.7671 - val_loss: 0.5671 - val_accuracy: 0.7811\n",
      "Epoch 133/1000\n",
      "36/36 [==============================] - 4s 125ms/step - loss: 0.5818 - accuracy: 0.7766 - val_loss: 0.6131 - val_accuracy: 0.7714\n",
      "Epoch 134/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.6041 - accuracy: 0.7675 - val_loss: 0.5901 - val_accuracy: 0.7829\n",
      "Epoch 135/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.5952 - accuracy: 0.7722 - val_loss: 0.5855 - val_accuracy: 0.7908\n",
      "Epoch 136/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.5987 - accuracy: 0.7669 - val_loss: 0.5584 - val_accuracy: 0.7935\n",
      "Epoch 137/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5756 - accuracy: 0.7728 - val_loss: 0.5883 - val_accuracy: 0.7617\n",
      "Epoch 138/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5897 - accuracy: 0.7762 - val_loss: 0.5694 - val_accuracy: 0.7864\n",
      "Epoch 139/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5893 - accuracy: 0.7742 - val_loss: 0.5564 - val_accuracy: 0.7873\n",
      "Epoch 140/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.5923 - accuracy: 0.7748 - val_loss: 0.5791 - val_accuracy: 0.7811\n",
      "Epoch 141/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5715 - accuracy: 0.7828 - val_loss: 0.5900 - val_accuracy: 0.7723\n",
      "Epoch 142/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5833 - accuracy: 0.7731 - val_loss: 0.5823 - val_accuracy: 0.7741\n",
      "Epoch 143/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5812 - accuracy: 0.7746 - val_loss: 0.5534 - val_accuracy: 0.7873\n",
      "Epoch 144/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5968 - accuracy: 0.7671 - val_loss: 0.5754 - val_accuracy: 0.7643\n",
      "Epoch 145/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5810 - accuracy: 0.7770 - val_loss: 0.5546 - val_accuracy: 0.8023\n",
      "Epoch 146/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5797 - accuracy: 0.7751 - val_loss: 0.5561 - val_accuracy: 0.7961\n",
      "Epoch 147/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5834 - accuracy: 0.7770 - val_loss: 0.5687 - val_accuracy: 0.7882\n",
      "Epoch 148/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5931 - accuracy: 0.7642 - val_loss: 0.5752 - val_accuracy: 0.7926\n",
      "Epoch 149/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5756 - accuracy: 0.7819 - val_loss: 0.5525 - val_accuracy: 0.7899\n",
      "Epoch 150/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5929 - accuracy: 0.7715 - val_loss: 0.5881 - val_accuracy: 0.7670\n",
      "Epoch 151/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5909 - accuracy: 0.7689 - val_loss: 0.6051 - val_accuracy: 0.7661\n",
      "Epoch 152/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5887 - accuracy: 0.7656 - val_loss: 0.5505 - val_accuracy: 0.7988\n",
      "Epoch 153/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5826 - accuracy: 0.7826 - val_loss: 0.5722 - val_accuracy: 0.7935\n",
      "Epoch 154/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5714 - accuracy: 0.7812 - val_loss: 0.5563 - val_accuracy: 0.7882\n",
      "Epoch 155/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5704 - accuracy: 0.7857 - val_loss: 0.5498 - val_accuracy: 0.7785\n",
      "Epoch 156/1000\n",
      "36/36 [==============================] - 1s 37ms/step - loss: 0.5683 - accuracy: 0.7863 - val_loss: 0.5332 - val_accuracy: 0.8041\n",
      "Epoch 157/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5642 - accuracy: 0.7870 - val_loss: 0.5527 - val_accuracy: 0.7961\n",
      "Epoch 158/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5726 - accuracy: 0.7792 - val_loss: 0.5453 - val_accuracy: 0.7996\n",
      "Epoch 159/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5566 - accuracy: 0.7834 - val_loss: 0.5611 - val_accuracy: 0.7811\n",
      "Epoch 160/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5616 - accuracy: 0.7804 - val_loss: 0.5567 - val_accuracy: 0.7785\n",
      "Epoch 161/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5520 - accuracy: 0.7898 - val_loss: 0.5887 - val_accuracy: 0.7696\n",
      "Epoch 162/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5525 - accuracy: 0.7845 - val_loss: 0.5643 - val_accuracy: 0.7970\n",
      "Epoch 163/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5543 - accuracy: 0.7907 - val_loss: 0.5445 - val_accuracy: 0.8041\n",
      "Epoch 164/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5758 - accuracy: 0.7709 - val_loss: 0.5578 - val_accuracy: 0.7988\n",
      "Epoch 165/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5884 - accuracy: 0.7664 - val_loss: 0.5692 - val_accuracy: 0.7873\n",
      "Epoch 166/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5617 - accuracy: 0.7799 - val_loss: 0.5784 - val_accuracy: 0.7776\n",
      "Epoch 167/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5659 - accuracy: 0.7788 - val_loss: 0.5311 - val_accuracy: 0.8005\n",
      "Epoch 168/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5442 - accuracy: 0.7863 - val_loss: 0.5102 - val_accuracy: 0.8058\n",
      "Epoch 169/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5469 - accuracy: 0.7916 - val_loss: 0.5700 - val_accuracy: 0.7758\n",
      "Epoch 170/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5570 - accuracy: 0.7865 - val_loss: 0.5264 - val_accuracy: 0.8005\n",
      "Epoch 171/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5574 - accuracy: 0.7863 - val_loss: 0.5855 - val_accuracy: 0.7820\n",
      "Epoch 172/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5497 - accuracy: 0.7907 - val_loss: 0.5179 - val_accuracy: 0.8208\n",
      "Epoch 173/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5411 - accuracy: 0.7901 - val_loss: 0.5244 - val_accuracy: 0.8076\n",
      "Epoch 174/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5603 - accuracy: 0.7806 - val_loss: 0.5303 - val_accuracy: 0.8085\n",
      "Epoch 175/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5401 - accuracy: 0.7989 - val_loss: 0.5632 - val_accuracy: 0.7882\n",
      "Epoch 176/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5615 - accuracy: 0.7777 - val_loss: 0.5638 - val_accuracy: 0.7802\n",
      "Epoch 177/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5395 - accuracy: 0.7907 - val_loss: 0.5299 - val_accuracy: 0.8102\n",
      "Epoch 178/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5388 - accuracy: 0.7923 - val_loss: 0.5338 - val_accuracy: 0.8155\n",
      "Epoch 179/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5283 - accuracy: 0.7945 - val_loss: 0.5405 - val_accuracy: 0.7793\n",
      "Epoch 180/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5333 - accuracy: 0.7901 - val_loss: 0.5432 - val_accuracy: 0.7785\n",
      "Epoch 181/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5607 - accuracy: 0.7781 - val_loss: 0.5289 - val_accuracy: 0.8147\n",
      "Epoch 182/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5340 - accuracy: 0.7956 - val_loss: 0.5132 - val_accuracy: 0.8155\n",
      "Epoch 183/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5276 - accuracy: 0.7947 - val_loss: 0.5837 - val_accuracy: 0.7785\n",
      "Epoch 184/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5491 - accuracy: 0.7830 - val_loss: 0.5423 - val_accuracy: 0.7882\n",
      "Epoch 185/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5471 - accuracy: 0.7806 - val_loss: 0.5060 - val_accuracy: 0.8023\n",
      "Epoch 186/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5413 - accuracy: 0.7872 - val_loss: 0.5623 - val_accuracy: 0.7979\n",
      "Epoch 187/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5356 - accuracy: 0.7898 - val_loss: 0.5185 - val_accuracy: 0.7917\n",
      "Epoch 188/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5370 - accuracy: 0.7927 - val_loss: 0.5132 - val_accuracy: 0.8032\n",
      "Epoch 189/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5508 - accuracy: 0.7885 - val_loss: 0.5338 - val_accuracy: 0.8208\n",
      "Epoch 190/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5390 - accuracy: 0.7916 - val_loss: 0.6373 - val_accuracy: 0.7776\n",
      "Epoch 191/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5809 - accuracy: 0.7664 - val_loss: 0.5365 - val_accuracy: 0.7952\n",
      "Epoch 192/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5229 - accuracy: 0.8051 - val_loss: 0.5053 - val_accuracy: 0.8094\n",
      "Epoch 193/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5140 - accuracy: 0.7987 - val_loss: 0.5534 - val_accuracy: 0.7970\n",
      "Epoch 194/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5630 - accuracy: 0.7799 - val_loss: 0.5498 - val_accuracy: 0.8005\n",
      "Epoch 195/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5373 - accuracy: 0.7978 - val_loss: 0.5192 - val_accuracy: 0.8147\n",
      "Epoch 196/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5276 - accuracy: 0.7976 - val_loss: 0.5181 - val_accuracy: 0.8191\n",
      "Epoch 197/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5328 - accuracy: 0.7929 - val_loss: 0.5330 - val_accuracy: 0.7917\n",
      "Epoch 198/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5353 - accuracy: 0.7960 - val_loss: 0.5281 - val_accuracy: 0.8005\n",
      "Epoch 199/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5220 - accuracy: 0.8004 - val_loss: 0.5143 - val_accuracy: 0.8147\n",
      "Epoch 200/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5126 - accuracy: 0.8060 - val_loss: 0.5627 - val_accuracy: 0.7714\n",
      "Epoch 201/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5243 - accuracy: 0.8002 - val_loss: 0.5815 - val_accuracy: 0.8076\n",
      "Epoch 202/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5391 - accuracy: 0.7852 - val_loss: 0.5535 - val_accuracy: 0.7855\n",
      "Epoch 203/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5232 - accuracy: 0.7949 - val_loss: 0.5159 - val_accuracy: 0.8173\n",
      "Epoch 204/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5203 - accuracy: 0.7996 - val_loss: 0.5289 - val_accuracy: 0.7952\n",
      "Epoch 205/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5219 - accuracy: 0.7958 - val_loss: 0.5114 - val_accuracy: 0.8049\n",
      "Epoch 206/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5259 - accuracy: 0.7929 - val_loss: 0.5303 - val_accuracy: 0.7944\n",
      "Epoch 207/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5137 - accuracy: 0.8011 - val_loss: 0.5120 - val_accuracy: 0.7979\n",
      "Epoch 208/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5170 - accuracy: 0.7940 - val_loss: 0.5052 - val_accuracy: 0.7952\n",
      "Epoch 209/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5199 - accuracy: 0.7985 - val_loss: 0.5193 - val_accuracy: 0.8085\n",
      "Epoch 210/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5235 - accuracy: 0.7980 - val_loss: 0.5271 - val_accuracy: 0.7855\n",
      "Epoch 211/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5112 - accuracy: 0.7989 - val_loss: 0.4953 - val_accuracy: 0.8244\n",
      "Epoch 212/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5114 - accuracy: 0.7987 - val_loss: 0.5272 - val_accuracy: 0.8014\n",
      "Epoch 213/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5121 - accuracy: 0.8033 - val_loss: 0.5080 - val_accuracy: 0.7970\n",
      "Epoch 214/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.4939 - accuracy: 0.8130 - val_loss: 0.5106 - val_accuracy: 0.7996\n",
      "Epoch 215/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5347 - accuracy: 0.7912 - val_loss: 0.4870 - val_accuracy: 0.8288\n",
      "Epoch 216/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5190 - accuracy: 0.7985 - val_loss: 0.5744 - val_accuracy: 0.7820\n",
      "Epoch 217/1000\n",
      "36/36 [==============================] - 1s 36ms/step - loss: 0.5140 - accuracy: 0.7996 - val_loss: 0.5036 - val_accuracy: 0.8076\n",
      "Epoch 218/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.5452 - accuracy: 0.7859 - val_loss: 0.5641 - val_accuracy: 0.7979\n",
      "Epoch 219/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5168 - accuracy: 0.7967 - val_loss: 0.5076 - val_accuracy: 0.8058\n",
      "Epoch 220/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.4893 - accuracy: 0.8117 - val_loss: 0.5130 - val_accuracy: 0.8049\n",
      "Epoch 221/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5154 - accuracy: 0.7998 - val_loss: 0.4882 - val_accuracy: 0.8367\n",
      "Epoch 222/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5140 - accuracy: 0.8018 - val_loss: 0.5540 - val_accuracy: 0.7838\n",
      "Epoch 223/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5421 - accuracy: 0.7936 - val_loss: 0.5307 - val_accuracy: 0.8076\n",
      "Epoch 224/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5055 - accuracy: 0.8086 - val_loss: 0.5039 - val_accuracy: 0.8111\n",
      "Epoch 225/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5210 - accuracy: 0.8042 - val_loss: 0.4959 - val_accuracy: 0.8147\n",
      "Epoch 226/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.4988 - accuracy: 0.8000 - val_loss: 0.5208 - val_accuracy: 0.7952\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 34ms/step - loss: 0.4937 - accuracy: 0.8060 - val_loss: 0.4936 - val_accuracy: 0.8208\n",
      "Epoch 228/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.4950 - accuracy: 0.8077 - val_loss: 0.4812 - val_accuracy: 0.8261\n",
      "Epoch 229/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.4915 - accuracy: 0.8057 - val_loss: 0.5059 - val_accuracy: 0.8138\n",
      "Epoch 230/1000\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.4932 - accuracy: 0.8082 - val_loss: 0.5187 - val_accuracy: 0.8014\n",
      "Epoch 231/1000\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.5063 - accuracy: 0.8000 - val_loss: 0.4953 - val_accuracy: 0.8208\n",
      "Epoch 232/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.4985 - accuracy: 0.8064 - val_loss: 0.5110 - val_accuracy: 0.8138\n",
      "Epoch 233/1000\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.4964 - accuracy: 0.8104 - val_loss: 0.4814 - val_accuracy: 0.8323\n",
      "Epoch 234/1000\n",
      "35/36 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.8114Restoring model weights from the end of the best epoch: 214.\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.5010 - accuracy: 0.8108 - val_loss: 0.5471 - val_accuracy: 0.8049\n",
      "Epoch 234: early stopping\n",
      "Overall execution time: 0 hours, 4 minutes, and 58 seconds\n"
     ]
    }
   ],
   "source": [
    "# records the starting time of the training process\n",
    "start_time = time.time()\n",
    "\n",
    "# monitors the validation accuracy during training ensuring the model's weights are restored to the \n",
    "# best epoch's weights when training stops after no improvement for a certain number of consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='accuracy', verbose=1, patience=20, mode='max', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(X_val_reshaped, y_val_encoded), callbacks=[early_stopping])\n",
    "\n",
    "# Calculate the execution time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Convert total seconds to hours, minutes, and seconds\n",
    "hours = int(execution_time // 3600)\n",
    "minutes = int((execution_time % 3600) // 60)\n",
    "seconds = int(execution_time % 60)\n",
    "\n",
    "# Display the execution time in hours, minutes, and seconds format\n",
    "print(\"Overall execution time: {} hours, {} minutes, and {} seconds\".format(hours, minutes, seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97f67eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunilamaharjan/miniconda3/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "# \".h5\" extension is commonly used for saving Keras models in Hierarchical Data Format (HDF5) format. \n",
    "# It's a binary file format that allows to store a wide range of data and metadata, including deep learning models\n",
    "saved_model_path = '/Users/sunilamaharjan/Desktop/London Metropolitan University/Summer Semester/mscProject/codes/Dataset/Number/sensor_data_training_model_withoutFs_3axis.h5'  \n",
    "model.save(saved_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb67811",
   "metadata": {},
   "source": [
    "# E. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf09bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 20ms/step - loss: 0.5171 - accuracy: 0.7945\n",
      "Test Loss: 0.5171424746513367\n",
      "Test Accuracy: 0.7944915294647217\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set (calculates the loss and metrics including accuracy)\n",
    "loss, accuracy = model.evaluate(X_test_reshaped, y_test_encoded)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef0eeefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91       159\n",
      "           1       0.70      0.77      0.73       129\n",
      "           2       0.79      0.69      0.73       147\n",
      "           3       0.87      0.33      0.48       139\n",
      "           4       0.59      0.80      0.68       140\n",
      "           5       0.72      0.83      0.77       143\n",
      "           6       0.81      0.81      0.81       142\n",
      "           7       0.85      0.89      0.87       129\n",
      "           8       0.88      0.92      0.90       132\n",
      "           9       0.99      0.92      0.96       156\n",
      "\n",
      "    accuracy                           0.79      1416\n",
      "   macro avg       0.81      0.79      0.78      1416\n",
      "weighted avg       0.81      0.79      0.79      1416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions for the testing set\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Convert the predictions to class labels by selecting the class with the highest probability for each sample\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate a classification report by comparing the true class labels (y_test) with the \n",
    "# predicted class labels (y_pred_labels).\n",
    "report = classification_report(y_test, y_pred_labels)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3636b129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "152b8c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7944915254237288\n"
     ]
    }
   ],
   "source": [
    "# calculates the accuracy of model's predictions by comparing the true labels (y_test) \n",
    "# with the predicted labels (y_pred_labels).\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3b5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
